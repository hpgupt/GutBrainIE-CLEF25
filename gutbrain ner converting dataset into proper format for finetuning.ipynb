{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b1r80Nv4bxdu"
      },
      "outputs": [],
      "source": [
        "%pip install tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NoFL_iktdZBI"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lDbmQWn8_Hyq"
      },
      "outputs": [],
      "source": [
        "PATH_PLATINUM_TRAIN = \"/content/train_platinum.json\"\n",
        "PATH_GOLD_TRAIN = \"/content/train_gold.json\"\n",
        "PATH_DEV = \"/content/dev.json\"\n",
        "OUTPUT_TRAIN_FILE = \"/content/gutbrain_finetune_train.jsonl\"\n",
        "OUTPUT_VALID_FILE = \"/content/gutbrain_finetune_valid.jsonl\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ICpCvrYO_OnG"
      },
      "outputs": [],
      "source": [
        "ORIGINAL_TO_UNDERSCORE_LABEL = {'anatomical location': 'anatomical_location', 'animal': 'animal', 'biomedical technique': 'biomedical_technique', 'bacteria': 'bacteria', 'chemical': 'chemical', 'dietary supplement': 'dietary_supplement', 'DDF': 'DDF', 'drug': 'drug', 'food': 'food', 'gene': 'gene', 'human': 'human', 'microbiome': 'microbiome', 'statistical technique': 'statistical_technique'}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0uy_QZqB_6fx"
      },
      "outputs": [],
      "source": [
        "UNDERSCORE_LABELS = list(ORIGINAL_TO_UNDERSCORE_LABEL.values())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c7i2Nxun_-ue"
      },
      "outputs": [],
      "source": [
        "def create_inline_prompt(text_to_annotate):\n",
        "    \"\"\"Creates the prompt for the LLM using inline annotation style.\"\"\"\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "Task Description:\n",
        "Your task is to identify and extract entities from the provided text. Perform Named Entity Recognition on the biomedical text provided in the \"Text\" section using a specific inline annotation style.\n",
        "Identify all mentions of entities belonging to the predefined categories listed below. An entity can occur multiple times; treat each occurrence as a separate entity and mark them directly within the text.\n",
        "Mark the beginning of an entity's span with \"@@\" and the end of the span with \"##\" followed immediately by the entity's category label. Note - the output text should be exactly identical to the input text i.e all spaces , special characters/ unicode characters,html/markdown tags,etc (or any other character) also should be exactly the same, except for the added \"@@, ## and entity label\" annotations for each entity detected.\n",
        "Text span of an entity means the actual words/characters that form the entity in the text. An entity's span can contain single or multiple words but never partial words.\n",
        "The format to follow while marking an entity with its label is  @@<entity_text_span>##<label>\n",
        "\n",
        "Predefined Entity Categories (use these exact labels after ##):\n",
        "[\n",
        "    \"anatomical_location\", \"animal\", \"biomedical_technique\", \"bacteria\",\n",
        "    \"chemical\", \"dietary_supplement\", \"DDF\", \"drug\", \"food\", \"gene\",\n",
        "    \"human\", \"microbiome\", \"statistical_technique\"\n",
        "]\n",
        "\n",
        "Note - DDF stands for Disease, Disorder, or Finding. The remaining categories refer to their conventional or scientific meaning.\n",
        "Also, If the first word or first set of words in output belong to an entity then ensure to start the output with @@ and follow rest of instructions.\n",
        "\n",
        "### Text\n",
        "Input: {text_to_annotate}\n",
        "Output:\"\"\"\n",
        "    return prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wfiYmF5oHCDs"
      },
      "outputs": [],
      "source": [
        "def annotate_text(original_text, entities):\n",
        "    if not entities:\n",
        "        return original_text\n",
        "\n",
        "    processed_entities = []\n",
        "    for e in entities:\n",
        "        if not all(k in e for k in ['start_idx', 'end_idx', 'text_span', 'label']):\n",
        "             print(f\"Skipping invalid entity structure: {e}\")\n",
        "             continue\n",
        "        try:\n",
        "            start = int(e['start_idx'])\n",
        "            end = int(e['end_idx']) + 1\n",
        "            label = e['label']\n",
        "            text_span = e['text_span']\n",
        "\n",
        "            if original_text[start:end] != text_span:\n",
        "                print(f\"Span mismatch! Expected '{original_text[start:end]}', found '{text_span}' at {start}:{end}. Skipping entity: {e}\")\n",
        "                continue\n",
        "\n",
        "            underscore_label = ORIGINAL_TO_UNDERSCORE_LABEL.get(label)\n",
        "            if not underscore_label:\n",
        "                print(f\"Label '{label}' not found in mapping. Skipping entity: {e}\")\n",
        "                continue\n",
        "\n",
        "            processed_entities.append({\n",
        "                'start': start,\n",
        "                'end': end,\n",
        "                'text_span': text_span,\n",
        "                'underscore_label': underscore_label\n",
        "            })\n",
        "        except (ValueError, TypeError) as ve:\n",
        "             print(f\"Error processing entity indices/label {e}: {ve}. Skipping.\")\n",
        "             continue\n",
        "        except IndexError:\n",
        "             print(f\"Index out of bounds for entity {e} in text of length {len(original_text)}. Skipping.\")\n",
        "             continue\n",
        "\n",
        "    processed_entities.sort(key=lambda x: x['start'])\n",
        "    annotated_text = \"\"\n",
        "    last_idx = 0\n",
        "\n",
        "    for entity in processed_entities:\n",
        "        start = entity['start']\n",
        "        end = entity['end']\n",
        "        text_span = entity['text_span']\n",
        "        underscore_label = entity['underscore_label']\n",
        "\n",
        "        if start < last_idx:\n",
        "            print(f\"Detected overlapping entity: '{text_span}' at {start} overlaps with previous entity ending at {last_idx-1}. Skipping overlap.\")\n",
        "            continue\n",
        "\n",
        "        annotated_text += original_text[last_idx:start]\n",
        "        annotated_text += f\"@@{text_span}##{underscore_label}\"\n",
        "        last_idx = end\n",
        "\n",
        "    annotated_text += original_text[last_idx:]\n",
        "    return annotated_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oiObPAPbIW_K"
      },
      "outputs": [],
      "source": [
        "def load_json_data(filepath):\n",
        "    try:\n",
        "        with open(filepath, 'r', encoding='utf-8') as f:\n",
        "            return json.load(f)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"File not found: {filepath}\")\n",
        "        return None\n",
        "    except json.JSONDecodeError:\n",
        "        print(f\"Error decoding JSON from: {filepath}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SqJqQMp6Ijba"
      },
      "outputs": [],
      "source": [
        "def create_finetuning_file(input_data, output_filepath, system_prompt):\n",
        "\n",
        "    processed_count = 0\n",
        "    skipped_docs = 0\n",
        "    with open(output_filepath, 'w', encoding='utf-8') as outfile:\n",
        "        for pmid, data in input_data.items():\n",
        "            try:\n",
        "                if \"metadata\" not in data or \"title\" not in data[\"metadata\"] or \"abstract\" not in data[\"metadata\"]:\n",
        "                    print(f\"Skipping PMID {pmid} due to missing metadata.\")\n",
        "                    skipped_docs += 1\n",
        "                    continue\n",
        "\n",
        "                title = data[\"metadata\"][\"title\"]\n",
        "                abstract = data[\"metadata\"][\"abstract\"]\n",
        "                all_entities = data.get(\"entities\", [])\n",
        "\n",
        "                title_entities = [e for e in all_entities if e.get(\"location\") == \"title\"]\n",
        "                abstract_entities = [e for e in all_entities if e.get(\"location\") == \"abstract\"]\n",
        "\n",
        "                if title:\n",
        "                    relative_title_entities = []\n",
        "                    for e in title_entities:\n",
        "                        try:\n",
        "                           rel_e = e.copy()\n",
        "                           relative_title_entities.append(rel_e)\n",
        "                        except KeyError as ke:\n",
        "                             print(f\"Missing key {ke} in title entity {e} for PMID {pmid}. Skipping entity.\")\n",
        "\n",
        "\n",
        "                    annotated_title = annotate_text(title, relative_title_entities)\n",
        "                    message = {\n",
        "                        \"messages\": [\n",
        "                            {\"role\": \"system\", \"content\": system_prompt},\n",
        "                            {\"role\": \"user\", \"content\": create_inline_prompt(title)},\n",
        "                            {\"role\": \"assistant\", \"content\": annotated_title}\n",
        "                        ]\n",
        "                    }\n",
        "                    outfile.write(json.dumps(message, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "                if abstract:\n",
        "                    relative_abstract_entities = []\n",
        "                    for e in abstract_entities:\n",
        "                         try:\n",
        "                            rel_e = e.copy()\n",
        "                            rel_e['start_idx'] = int(e['start_idx'])\n",
        "                            rel_e['end_idx'] = int(e['end_idx'])\n",
        "                            if rel_e['start_idx'] < 0 or rel_e['end_idx'] >= len(abstract):\n",
        "                                 print(f\"Invalid relative index for abstract entity {e} (relative: {rel_e['start_idx']}:{rel_e['end_idx']}) in PMID {pmid}. Skipping entity.\")\n",
        "                                 continue\n",
        "                            relative_abstract_entities.append(rel_e)\n",
        "                         except (KeyError, ValueError, TypeError) as err:\n",
        "                             print(f\"Error adjusting abstract entity {e} for PMID {pmid}: {err}. Skipping entity.\")\n",
        "\n",
        "\n",
        "                    annotated_abstract = annotate_text(abstract, relative_abstract_entities)\n",
        "                    message = {\n",
        "                        \"messages\": [\n",
        "                            {\"role\": \"system\", \"content\": system_prompt},\n",
        "                            {\"role\": \"user\", \"content\": create_inline_prompt(abstract)},\n",
        "                            {\"role\": \"assistant\", \"content\": annotated_abstract}\n",
        "                        ]\n",
        "                    }\n",
        "                    outfile.write(json.dumps(message, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "                processed_count += 1\n",
        "                if processed_count % 100 == 0:\n",
        "                    print(f\"Processed {processed_count} documents...\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Unexpected error processing PMID {pmid}: {e}\")\n",
        "                skipped_docs += 1\n",
        "\n",
        "    print(f\"Finished writing to {output_filepath}.\")\n",
        "    print(f\"Total documents processed: {processed_count}\")\n",
        "    print(f\"Total documents skipped: {skipped_docs}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3LlPiW11K7pB",
        "outputId": "6f6f05b5-7f28-42e2-d892-d86450ce1277"
      },
      "outputs": [],
      "source": [
        "print(\"Loading datasets...\")\n",
        "train_platinum_data = load_json_data(PATH_PLATINUM_TRAIN)\n",
        "train_gold_data = load_json_data(PATH_GOLD_TRAIN)\n",
        "valid_data = load_json_data(PATH_DEV)\n",
        "train_data = {}\n",
        "train_data.update(train_platinum_data)\n",
        "train_data.update(train_gold_data)\n",
        "print(f\"Combined training data: {len(train_data)} documents.\")\n",
        "print(f\"Validation data: {len(valid_data)} documents.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "50hrPrsULRvL",
        "outputId": "fd9bc345-373c-4b25-de4b-a1279529a2ab"
      },
      "outputs": [],
      "source": [
        "SYSTEM_PROMPT = \"You are an expert Named Entity Recognition (NER) system specializing in biomedical texts related to the gut-brain axis.\"\n",
        "create_finetuning_file(train_data, OUTPUT_TRAIN_FILE, SYSTEM_PROMPT)\n",
        "create_finetuning_file(valid_data, OUTPUT_VALID_FILE, SYSTEM_PROMPT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6tUvZ5Lbp5-"
      },
      "source": [
        "Validate files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ro3KTcuYbPZq"
      },
      "outputs": [],
      "source": [
        "import tiktoken\n",
        "import numpy as np\n",
        "from collections import defaultdict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vW1uRqeMb3kZ",
        "outputId": "21ed5ab5-e8bd-4d79-d4c1-2936a22fb097"
      },
      "outputs": [],
      "source": [
        "def get_stats(path):\n",
        "  with open(path, 'r', encoding='utf-8') as f:\n",
        "      dataset = [json.loads(line) for line in f]\n",
        "\n",
        "  # Initial dataset stats\n",
        "  print(\"Num examples:\", len(dataset))\n",
        "  print(\"First example:\")\n",
        "  for message in dataset[0][\"messages\"]:\n",
        "      print(message)\n",
        "  return dataset\n",
        "\n",
        "train_set = get_stats(OUTPUT_TRAIN_FILE)\n",
        "val_set = get_stats(OUTPUT_VALID_FILE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SVqbuXShcY0g",
        "outputId": "f87a3e4a-2250-42d3-e9da-f1c366b8a79a"
      },
      "outputs": [],
      "source": [
        "def check_for_errors(dataset):\n",
        "  format_errors = defaultdict(int)\n",
        "  for ex in dataset:\n",
        "      if not isinstance(ex, dict):\n",
        "          format_errors[\"data_type\"] += 1\n",
        "          continue\n",
        "\n",
        "      messages = ex.get(\"messages\", None)\n",
        "      if not messages:\n",
        "          format_errors[\"missing_messages_list\"] += 1\n",
        "          continue\n",
        "\n",
        "      for message in messages:\n",
        "          if \"role\" not in message or \"content\" not in message:\n",
        "              format_errors[\"message_missing_key\"] += 1\n",
        "\n",
        "          if any(k not in (\"role\", \"content\", \"name\", \"function_call\", \"weight\") for k in message):\n",
        "              format_errors[\"message_unrecognized_key\"] += 1\n",
        "\n",
        "          if message.get(\"role\", None) not in (\"system\", \"user\", \"assistant\", \"function\"):\n",
        "              format_errors[\"unrecognized_role\"] += 1\n",
        "\n",
        "          content = message.get(\"content\", None)\n",
        "          function_call = message.get(\"function_call\", None)\n",
        "\n",
        "          if (not content and not function_call) or not isinstance(content, str):\n",
        "              format_errors[\"missing_content\"] += 1\n",
        "\n",
        "      if not any(message.get(\"role\", None) == \"assistant\" for message in messages):\n",
        "          format_errors[\"example_missing_assistant_message\"] += 1\n",
        "\n",
        "  if format_errors:\n",
        "      print(\"Found errors:\")\n",
        "      for k, v in format_errors.items():\n",
        "          print(f\"{k}: {v}\")\n",
        "  else:\n",
        "      print(\"No errors found\")\n",
        "\n",
        "check_for_errors(train_set)\n",
        "check_for_errors(val_set)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o8kEC1EccsLe"
      },
      "outputs": [],
      "source": [
        "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
        "def num_tokens_from_messages(messages, tokens_per_message=3, tokens_per_name=1):\n",
        "    num_tokens = 0\n",
        "    for message in messages:\n",
        "        num_tokens += tokens_per_message\n",
        "        for key, value in message.items():\n",
        "            num_tokens += len(encoding.encode(value))\n",
        "            if key == \"name\":\n",
        "                num_tokens += tokens_per_name\n",
        "    num_tokens += 3\n",
        "    return num_tokens\n",
        "\n",
        "def num_assistant_tokens_from_messages(messages):\n",
        "    num_tokens = 0\n",
        "    for message in messages:\n",
        "        if message[\"role\"] == \"assistant\":\n",
        "            num_tokens += len(encoding.encode(message[\"content\"]))\n",
        "    return num_tokens\n",
        "\n",
        "def print_distribution(values, name):\n",
        "    print(f\"\\n#### Distribution of {name}:\")\n",
        "    print(f\"min / max: {min(values)}, {max(values)}\")\n",
        "    print(f\"mean / median: {np.mean(values)}, {np.median(values)}\")\n",
        "    print(f\"p5 / p95: {np.quantile(values, 0.1)}, {np.quantile(values, 0.9)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y99Vr8Chc-GZ",
        "outputId": "cfa1bcc1-8c1f-48a2-9537-1cdebc9330dd"
      },
      "outputs": [],
      "source": [
        "def get_counts(dataset):\n",
        "  n_missing_system = 0\n",
        "  n_missing_user = 0\n",
        "  n_messages = []\n",
        "  convo_lens = []\n",
        "  assistant_message_lens = []\n",
        "\n",
        "  for ex in dataset:\n",
        "      messages = ex[\"messages\"]\n",
        "      if not any(message[\"role\"] == \"system\" for message in messages):\n",
        "          n_missing_system += 1\n",
        "      if not any(message[\"role\"] == \"user\" for message in messages):\n",
        "          n_missing_user += 1\n",
        "      n_messages.append(len(messages))\n",
        "      convo_lens.append(num_tokens_from_messages(messages))\n",
        "      assistant_message_lens.append(num_assistant_tokens_from_messages(messages))\n",
        "\n",
        "  print(\"Num examples missing system message:\", n_missing_system)\n",
        "  print(\"Num examples missing user message:\", n_missing_user)\n",
        "  print_distribution(n_messages, \"num_messages_per_example\")\n",
        "  print_distribution(convo_lens, \"num_total_tokens_per_example\")\n",
        "  print_distribution(assistant_message_lens, \"num_assistant_tokens_per_example\")\n",
        "  n_too_long = sum(l > 10000 for l in convo_lens)\n",
        "  print(f\"\\n{n_too_long} examples may be over the 10,000 token limit, they will be truncated during fine-tuning\")\n",
        "  return convo_lens\n",
        "\n",
        "train_convo_lens = get_counts(train_set)\n",
        "val_convo_lens = get_counts(val_set)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ILfIl8GNdk-X",
        "outputId": "8d7e16cb-420e-463c-87cd-180de0fccc5e"
      },
      "outputs": [],
      "source": [
        "# Pricing and default n_epochs estimate\n",
        "MAX_TOKENS_PER_EXAMPLE = 16385\n",
        "TARGET_EPOCHS = 5\n",
        "MIN_TARGET_EXAMPLES = 100\n",
        "MAX_TARGET_EXAMPLES = 400\n",
        "MIN_DEFAULT_EPOCHS = 5\n",
        "MAX_DEFAULT_EPOCHS = 5\n",
        "\n",
        "n_epochs = TARGET_EPOCHS\n",
        "n_train_examples = len(train_set)\n",
        "if n_train_examples * TARGET_EPOCHS < MIN_TARGET_EXAMPLES:\n",
        "    n_epochs = min(MAX_DEFAULT_EPOCHS, MIN_TARGET_EXAMPLES // n_train_examples)\n",
        "elif n_train_examples * TARGET_EPOCHS > MAX_TARGET_EXAMPLES:\n",
        "    n_epochs = max(MIN_DEFAULT_EPOCHS, MAX_TARGET_EXAMPLES // n_train_examples)\n",
        "\n",
        "n_billing_tokens_in_dataset = sum(min(MAX_TOKENS_PER_EXAMPLE, length) for length in train_convo_lens)\n",
        "print(f\"Dataset has ~{n_billing_tokens_in_dataset} tokens that will be charged for during training\")\n",
        "print(f\"By default, you'll train for {n_epochs} epochs on this dataset\")\n",
        "print(f\"By default, you'll be charged for ~{n_epochs * n_billing_tokens_in_dataset} tokens\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
