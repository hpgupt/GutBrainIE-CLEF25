{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O_Q1lt3Qtv-B"
      },
      "outputs": [],
      "source": [
        "!pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gMafgy4_OwpC"
      },
      "outputs": [],
      "source": [
        "!pip install faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O-J2fc0OuEOy"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "from openai import OpenAI\n",
        "from getpass import getpass\n",
        "import re\n",
        "import faiss\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BFqor6yhuKIK",
        "outputId": "d74b747d-564c-4f83-f885-84e86bc398ad"
      },
      "outputs": [],
      "source": [
        "key = getpass(\"Enter your OpenAI API key: \")\n",
        "os.environ[\"OPENAI_API_KEY\"] = key\n",
        "client = OpenAI()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wyimUDf4uQYz"
      },
      "outputs": [],
      "source": [
        "OPENAI_MODEL = \"ft:gpt-4.1-mini-2025-04-14:personal:gutbrain4:BUwx1WDM:ckpt-step-638\"\n",
        "TEMPERATURE = 0.0\n",
        "TOP_P = 1.0\n",
        "MAX_TOKENS = 16384\n",
        "DEV_FILE_PATH = \"/content/dev.json\"\n",
        "OUTPUT_FILE_PATH = \"/content/llm_ner_predictions.json\"\n",
        "TRAIN_FILE_PATH = \"/content/train.json\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tF0jRcrmup9J"
      },
      "outputs": [],
      "source": [
        "UNDERSCORE_LABELS = [\n",
        "    \"anatomical_location\", \"animal\", \"biomedical_technique\", \"bacteria\",\n",
        "    \"chemical\", \"dietary_supplement\", \"DDF\", \"drug\", \"food\", \"gene\",\n",
        "    \"human\", \"microbiome\", \"statistical_technique\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3owjqNl_uw9f"
      },
      "outputs": [],
      "source": [
        "UNDERSCORE_TO_ORIGINAL_LABEL = {\n",
        "    \"anatomical_location\": \"anatomical location\",\n",
        "    \"animal\": \"animal\",\n",
        "    \"biomedical_technique\": \"biomedical technique\",\n",
        "    \"bacteria\": \"bacteria\",\n",
        "    \"chemical\": \"chemical\",\n",
        "    \"dietary_supplement\": \"dietary supplement\",\n",
        "    \"DDF\": \"DDF\",\n",
        "    \"drug\": \"drug\",\n",
        "    \"food\": \"food\",\n",
        "    \"gene\": \"gene\",\n",
        "    \"human\": \"human\",\n",
        "    \"microbiome\": \"microbiome\",\n",
        "    \"statistical_technique\": \"statistical technique\"\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KCyUn0fiP362"
      },
      "outputs": [],
      "source": [
        "ORIGINAL_TO_UNDERSCORE_LABEL = {'anatomical location': 'anatomical_location', 'animal': 'animal', 'biomedical technique': 'biomedical_technique', 'bacteria': 'bacteria', 'chemical': 'chemical', 'dietary supplement': 'dietary_supplement', 'DDF': 'DDF', 'drug': 'drug', 'food': 'food', 'gene': 'gene', 'human': 'human', 'microbiome': 'microbiome', 'statistical technique': 'statistical_technique'}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zX7STbTPNPE"
      },
      "source": [
        "embeddings and vector db"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n9ye-WlpPQBH"
      },
      "outputs": [],
      "source": [
        "EMBEDDING_MODEL = \"text-embedding-3-large\"\n",
        "FAISS_INDEX_FILE = \"/content/gutbrain_train.index\"\n",
        "FAISS_METADATA_FILE = \"/content/gutbrain_train_metadata.json\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BMQkbIPKPSpb"
      },
      "outputs": [],
      "source": [
        "def load_json_data(filepath):\n",
        "    try:\n",
        "        with open(filepath, 'r', encoding='utf-8') as f:\n",
        "            return json.load(f)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"File not found: {filepath}\")\n",
        "        return None\n",
        "    except json.JSONDecodeError:\n",
        "        print(f\"Error decoding JSON from: {filepath}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qWn97v9vPlDV"
      },
      "outputs": [],
      "source": [
        "def get_embedding(text, model=EMBEDDING_MODEL):\n",
        "    try:\n",
        "        text = text.replace(\"\\n\", \" \")\n",
        "        response = client.embeddings.create(input=[text], model=model)\n",
        "        return response.data[0].embedding\n",
        "    except Exception as e:\n",
        "        print(f\"Error getting embedding for text '{text[:50]}...': {e}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h0N0Ne1BPtyj"
      },
      "outputs": [],
      "source": [
        "def annotate_text(original_text, entities):\n",
        "    if not entities:\n",
        "        return original_text\n",
        "\n",
        "    processed_entities = []\n",
        "    for e in entities:\n",
        "        if not all(k in e for k in ['start_idx', 'end_idx', 'text_span', 'label']):\n",
        "             print(f\"Skipping invalid entity structure: {e}\")\n",
        "             continue\n",
        "        try:\n",
        "            start = int(e['start_idx'])\n",
        "            end = int(e['end_idx']) + 1\n",
        "            label = e['label']\n",
        "            text_span = e['text_span']\n",
        "\n",
        "            if original_text[start:end] != text_span:\n",
        "                print(f\"Span mismatch! Expected '{original_text[start:end]}', found '{text_span}' at {start}:{end}. Skipping entity: {e}\")\n",
        "                continue\n",
        "\n",
        "            underscore_label = ORIGINAL_TO_UNDERSCORE_LABEL.get(label)\n",
        "            if not underscore_label:\n",
        "                print(f\"Label '{label}' not found in mapping. Skipping entity: {e}\")\n",
        "                continue\n",
        "\n",
        "            processed_entities.append({\n",
        "                'start': start,\n",
        "                'end': end,\n",
        "                'text_span': text_span,\n",
        "                'underscore_label': underscore_label\n",
        "            })\n",
        "        except (ValueError, TypeError) as ve:\n",
        "             print(f\"Error processing entity indices/label {e}: {ve}. Skipping.\")\n",
        "             continue\n",
        "        except IndexError:\n",
        "             print(f\"Index out of bounds for entity {e} in text of length {len(original_text)}. Skipping.\")\n",
        "             continue\n",
        "\n",
        "    processed_entities.sort(key=lambda x: x['start'])\n",
        "    annotated_text = \"\"\n",
        "    last_idx = 0\n",
        "\n",
        "    for entity in processed_entities:\n",
        "        start = entity['start']\n",
        "        end = entity['end']\n",
        "        text_span = entity['text_span']\n",
        "        underscore_label = entity['underscore_label']\n",
        "\n",
        "        if start < last_idx:\n",
        "            print(f\"Detected overlapping entity: '{text_span}' at {start} overlaps with previous entity ending at {last_idx-1}. Skipping overlap.\")\n",
        "            continue\n",
        "\n",
        "        annotated_text += original_text[last_idx:start]\n",
        "        annotated_text += f\"@@{text_span}##{underscore_label}\"\n",
        "        last_idx = end\n",
        "\n",
        "    annotated_text += original_text[last_idx:]\n",
        "    return annotated_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pAkjIwHvQRgA"
      },
      "outputs": [],
      "source": [
        "def setup_vector_db(train_data_path, index_path, metadata_path, force_recreate=False):\n",
        "    \"\"\"\n",
        "    Creates or loads a FAISS index and metadata for the training data.\n",
        "    Stores pmid, location (title/abstract), original text, and its embedding.\n",
        "    \"\"\"\n",
        "    if not force_recreate and os.path.exists(index_path) and os.path.exists(metadata_path):\n",
        "        print(f\"Loading existing FAISS index from {index_path} and metadata from {metadata_path}\")\n",
        "        index = faiss.read_index(index_path)\n",
        "        with open(metadata_path, 'r', encoding='utf-8') as f:\n",
        "            metadata = json.load(f)\n",
        "        return index, metadata\n",
        "\n",
        "    print(\"Creating new FAISS index and metadata...\")\n",
        "    train_data = load_json_data(train_data_path)\n",
        "    if not train_data:\n",
        "        raise ValueError(\"Failed to load training data for vector DB setup.\")\n",
        "\n",
        "    embeddings_list = []\n",
        "    metadata = []  # List of dicts: {'pmid', 'location', 'text'}\n",
        "\n",
        "    doc_id_counter = 0\n",
        "    for pmid, data in train_data.items():\n",
        "        if \"metadata\" not in data:\n",
        "            print(f\"Skipping PMID {pmid} in DB setup: no metadata.\")\n",
        "            continue\n",
        "\n",
        "        title = data[\"metadata\"].get(\"title\")\n",
        "        abstract = data[\"metadata\"].get(\"abstract\")\n",
        "\n",
        "        # Process Title\n",
        "        if title:\n",
        "            title_embedding = get_embedding(title)\n",
        "            if title_embedding:\n",
        "                embeddings_list.append(title_embedding)\n",
        "                metadata.append({\n",
        "                    \"id\": doc_id_counter,\n",
        "                    \"pmid\": pmid,\n",
        "                    \"location\": \"title\",\n",
        "                    \"text\": title\n",
        "                })\n",
        "                doc_id_counter += 1\n",
        "\n",
        "        if abstract:\n",
        "            abstract_embedding = get_embedding(abstract)\n",
        "            if abstract_embedding:\n",
        "                embeddings_list.append(abstract_embedding)\n",
        "                metadata.append({\n",
        "                    \"id\": doc_id_counter,\n",
        "                    \"pmid\": pmid,\n",
        "                    \"location\": \"abstract\",\n",
        "                    \"text\": abstract\n",
        "                })\n",
        "                doc_id_counter += 1\n",
        "\n",
        "\n",
        "    if not embeddings_list:\n",
        "        print(\"No embeddings were generated. Cannot create FAISS index.\")\n",
        "        return None, None\n",
        "\n",
        "    embeddings_np = np.array(embeddings_list).astype('float32')\n",
        "    dimension = embeddings_np.shape[1]\n",
        "    index = faiss.IndexFlatL2(dimension)\n",
        "    index.add(embeddings_np)\n",
        "\n",
        "    print(f\"FAISS index created with {index.ntotal} vectors of dimension {dimension}.\")\n",
        "    faiss.write_index(index, index_path)\n",
        "    with open(metadata_path, 'w', encoding='utf-8') as f:\n",
        "        json.dump(metadata, f, ensure_ascii=False, indent=2)\n",
        "    print(f\"FAISS index saved to {index_path}, metadata to {metadata_path}\")\n",
        "\n",
        "    return index, metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "06u2ItvPvBiF"
      },
      "outputs": [],
      "source": [
        "def base_inline_prompt():\n",
        "    \"\"\"Creates the base prompt for the LLM using inline annotation style.\"\"\"\n",
        "\n",
        "    prompt = f\"\"\"Perform Named Entity Recognition on the biomedical text provided in the \"Text\" section using a specific inline annotation style.\n",
        "\n",
        "Task Description:\n",
        "You are an expert Named Entity Recognition (NER) system specializing in biomedical texts related to the gut-brain axis. Your task is to identify and extract entities from the provided text.\n",
        "Identify all mentions of entities belonging to the predefined categories listed below. An entity can occur multiple times; treat each occurrence as a separate entity and mark them directly within the text.\n",
        "Mark the beginning of an entity's span with \"@@\" and the end of the span with \"##\" followed immediately by the entity's category label. Note - the output text should be exactly identical to the input text i.e all spaces , special characters/ unicode characters,html/markdown tags,etc (any character) also should be exactly the same, except for the added \"@@, ## and entity label\" annotations for each entity detected.\n",
        "Text span of an entity means the actual words/characters that form the entity in the text. An entity's span can contain single or multiple words but never partial words.\n",
        "The format to follow while marking an entity with its label is  @@<entity_text_span>##<label>\n",
        "\n",
        "Predefined Entity Categories (use these exact labels after ##):\n",
        "[\n",
        "    \"anatomical_location\", \"animal\", \"biomedical_technique\", \"bacteria\",\n",
        "    \"chemical\", \"dietary_supplement\", \"DDF\", \"drug\", \"food\", \"gene\",\n",
        "    \"human\", \"microbiome\", \"statistical_technique\"\n",
        "]\n",
        "\n",
        "Note - DDF stands for Disease, Disorder, or Finding. The remaining categories refer to their conventional or scientific meaning.\n",
        "Also, If the first word or first set of words in output belong to an entity then ensure to start the output with @@ and follow rest of instructions.\n",
        "\"\"\"\n",
        "    return prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U3wu34DXS2mw"
      },
      "outputs": [],
      "source": [
        "def create_few_shot_inline_prompt(\n",
        "    text_to_annotate,\n",
        "    num_shots,\n",
        "    faiss_index,\n",
        "    faiss_metadata,\n",
        "    full_train_data,\n",
        "    openai_client,\n",
        "    current_pmid,\n",
        "    current_location\n",
        "):\n",
        "    \"\"\"\n",
        "    Creates a prompt for the LLM, dynamically injecting few-shot examples\n",
        "    based on vector similarity.\n",
        "    \"\"\"\n",
        "    # Base prompt structure (0-shot)\n",
        "    base_prompt = base_inline_prompt()\n",
        "    examples_section = \"Follow the format shown in the detailed examples below precisely. \\n ### Examples\\n\"\n",
        "    final_text_section = f\"### Text\\nInput: {text_to_annotate}\\nOutput:\"\n",
        "\n",
        "    if num_shots == 0:\n",
        "        return base_prompt + \"\\n\" + final_text_section\n",
        "\n",
        "    if not faiss_index or not faiss_metadata:\n",
        "        print(\"FAISS index or metadata not available for few-shot example selection. Falling back to 0-shot.\")\n",
        "        return base_prompt + \"\\n\" + final_text_section\n",
        "\n",
        "    current_text_embedding = get_embedding(text_to_annotate, model=EMBEDDING_MODEL)\n",
        "    if current_text_embedding is None:\n",
        "        print(\"Could not generate embedding for current text. Falling back to 0-shot.\")\n",
        "        return base_prompt + \"\\n\" + final_text_section\n",
        "\n",
        "    query_embedding = np.array([current_text_embedding]).astype('float32')\n",
        "\n",
        "    num_abstracts_needed, num_titles_needed = 0, 0\n",
        "    if num_shots == 1:\n",
        "        num_abstracts_needed = 1\n",
        "    elif num_shots == 3:\n",
        "        num_abstracts_needed = 2\n",
        "        num_titles_needed = 1\n",
        "    elif num_shots == 5:\n",
        "        num_abstracts_needed = 3\n",
        "        num_titles_needed = 2\n",
        "    else:\n",
        "        print(f\"Invalid num_shots: {num_shots}. Defaulting to 0-shot.\")\n",
        "        return base_prompt + \"\\n\" + final_text_section\n",
        "\n",
        "    k_to_fetch = 500\n",
        "    distances, indices = faiss_index.search(query_embedding, k_to_fetch)\n",
        "\n",
        "    selected_examples_formatted = []\n",
        "    selected_example_keys = set() # To avoid duplicate examples if they are very similar\n",
        "\n",
        "    # Collect candidates\n",
        "    abstract_candidates = []\n",
        "    title_candidates = []\n",
        "\n",
        "    for i in range(len(indices[0])):\n",
        "        retrieved_idx = indices[0][i]\n",
        "        if retrieved_idx < 0 or retrieved_idx >= len(faiss_metadata):\n",
        "            continue\n",
        "\n",
        "        example_meta = faiss_metadata[retrieved_idx]\n",
        "        example_pmid = example_meta[\"pmid\"]\n",
        "        example_location = example_meta[\"location\"]\n",
        "        example_key = (example_pmid, example_location)\n",
        "\n",
        "        if example_pmid == current_pmid and example_location == current_location:\n",
        "            continue\n",
        "        if example_key in selected_example_keys:\n",
        "            continue\n",
        "\n",
        "        if example_location == \"abstract\":\n",
        "            abstract_candidates.append(example_meta)\n",
        "        elif example_location == \"title\":\n",
        "            title_candidates.append(example_meta)\n",
        "\n",
        "    final_selected_metadata = []\n",
        "    for _ in range(num_abstracts_needed):\n",
        "        if abstract_candidates:\n",
        "            meta = abstract_candidates.pop(0)\n",
        "            final_selected_metadata.append(meta)\n",
        "            selected_example_keys.add((meta[\"pmid\"], meta[\"location\"]))\n",
        "    for _ in range(num_titles_needed):\n",
        "        if title_candidates:\n",
        "            meta = title_candidates.pop(0)\n",
        "            final_selected_metadata.append(meta)\n",
        "            selected_example_keys.add((meta[\"pmid\"], meta[\"location\"]))\n",
        "\n",
        "    # needed_more = num_shots - len(final_selected_metadata)\n",
        "    # if needed_more > 0:\n",
        "    #     remaining_candidates = abstract_candidates + title_candidates\n",
        "    #     for _ in range(needed_more):\n",
        "    #         if remaining_candidates:\n",
        "    #             meta = remaining_candidates.pop(0)\n",
        "    #             if (meta[\"pmid\"], meta[\"location\"]) not in selected_example_keys:\n",
        "    #                 final_selected_metadata.append(meta)\n",
        "    #                 selected_example_keys.add((meta[\"pmid\"], meta[\"location\"]))\n",
        "\n",
        "\n",
        "    # Format the selected examples\n",
        "    for example_meta in final_selected_metadata:\n",
        "        example_pmid = example_meta[\"pmid\"]\n",
        "        example_location = example_meta[\"location\"]\n",
        "        example_text = example_meta[\"text\"]\n",
        "\n",
        "        doc_data = full_train_data.get(example_pmid)\n",
        "        if not doc_data or \"entities\" not in doc_data:\n",
        "            print(f\"Could not find entity data for example PMID {example_pmid}. Skipping example.\")\n",
        "            continue\n",
        "\n",
        "        all_doc_entities = doc_data[\"entities\"]\n",
        "        example_entities = []\n",
        "\n",
        "        if example_location == \"title\":\n",
        "            example_entities = [e for e in all_doc_entities if e.get(\"location\") == \"title\"]\n",
        "            # Indices are already relative to title if location is title\n",
        "        elif example_location == \"abstract\":\n",
        "            # Adjust abstract entity indices to be relative to abstract text\n",
        "            for e in all_doc_entities:\n",
        "                if e.get(\"location\") == \"abstract\":\n",
        "                    try:\n",
        "                        rel_e = e.copy()\n",
        "                        rel_e['start_idx'] = int(e['start_idx'])\n",
        "                        rel_e['end_idx'] = int(e['end_idx'])\n",
        "                        if rel_e['start_idx'] >= 0 and rel_e['end_idx'] < len(example_text):\n",
        "                             example_entities.append(rel_e)\n",
        "                        else:\n",
        "                             print(f\"Adjusted entity out of bounds for example {example_pmid}/{example_location}: {rel_e}\")\n",
        "                    except (KeyError, ValueError, TypeError) as err:\n",
        "                        print(f\"Error adjusting entity for example {example_pmid}/{example_location}: {err}\")\n",
        "\n",
        "\n",
        "        annotated_example_output = annotate_text(example_text, example_entities)\n",
        "        selected_examples_formatted.append(f\"Input: {example_text}\\nOutput: {annotated_example_output}\")\n",
        "\n",
        "    if not selected_examples_formatted:\n",
        "        print(\"No valid few-shot examples could be generated. Falling back to 0-shot.\")\n",
        "        return base_prompt + \"\\n\" + final_text_section\n",
        "\n",
        "    examples_str = \"\\n\".join(selected_examples_formatted)\n",
        "    return base_prompt + \"\\n\" + examples_section + examples_str + \"\\n\\n\" + final_text_section\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PFCvPcpdXG8N",
        "outputId": "8ef17471-52db-4867-c7d8-bbd1ac9a3116"
      },
      "outputs": [],
      "source": [
        "full_train_data = load_json_data(TRAIN_FILE_PATH)\n",
        "faiss_index_global, faiss_metadata_global = setup_vector_db(\n",
        "            TRAIN_FILE_PATH,\n",
        "            FAISS_INDEX_FILE,\n",
        "            FAISS_METADATA_FILE,\n",
        "            force_recreate=False\n",
        "        )\n",
        "if faiss_index_global and faiss_metadata_global:\n",
        "  print(\"FAISS Index and metadata are ready.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "quAuonA6WcTZ"
      },
      "source": [
        "eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c-vZdmnnsXpn"
      },
      "outputs": [],
      "source": [
        "def parse_llm_output(llm_generated_text, original_text, location, threshold = 10):\n",
        "    label_pattern = \"|\".join(re.escape(lbl) for lbl in UNDERSCORE_TO_ORIGINAL_LABEL.keys())\n",
        "\n",
        "    # 1. Clean stray markers not matching '@@...##<label>'\n",
        "    # Remove '@@' not followed by valid content and ##label\n",
        "    stray_at_pattern = r\"@@(?!(?:[^#]+)##(?:\" + label_pattern + r\"))\"\n",
        "    stray_hash_pattern = r\"##(?!((?:\" + label_pattern + r\")))\"\n",
        "    cleaned_text = re.sub(stray_at_pattern, \"\", llm_generated_text)\n",
        "    cleaned_text = re.sub(stray_hash_pattern, \"\", cleaned_text)\n",
        "\n",
        "    # 2. Find all valid annotations\n",
        "    pattern = re.compile(r\"@@([^#]+?)##(\" + label_pattern + r\")\")\n",
        "    entities = []\n",
        "    cumulative_offset = 0\n",
        "\n",
        "    for match in pattern.finditer(cleaned_text):\n",
        "        span_text = match.group(1)\n",
        "        underscore_label = match.group(2)\n",
        "        original_label = UNDERSCORE_TO_ORIGINAL_LABEL.get(underscore_label)\n",
        "        if not original_label:\n",
        "            print(f\"Unknown label '{underscore_label}' in LLM output for {location}. Skipping.\")\n",
        "            cumulative_offset += len(match.group(0)) - len(span_text)\n",
        "            continue\n",
        "\n",
        "        # Estimate start/end in original text\n",
        "        raw_start = match.start() - cumulative_offset\n",
        "        raw_end = raw_start + len(span_text) - 1\n",
        "\n",
        "        def slice_matches(s, e):\n",
        "            return 0 <= s <= e < len(original_text) and original_text[s:e+1] == span_text\n",
        "\n",
        "        if slice_matches(raw_start, raw_end):\n",
        "            start_idx, end_idx = raw_start, raw_end\n",
        "        else:\n",
        "            found = False\n",
        "            for shift in range(1, threshold + 1):\n",
        "                s_r = raw_start + shift\n",
        "                e_r = s_r + len(span_text) - 1\n",
        "                if slice_matches(s_r, e_r):\n",
        "                    start_idx, end_idx = s_r, e_r\n",
        "                    found = True\n",
        "                    break\n",
        "                s_l = raw_start - shift\n",
        "                e_l = s_l + len(span_text) - 1\n",
        "                if slice_matches(s_l, e_l):\n",
        "                    start_idx, end_idx = s_l, e_l\n",
        "                    found = True\n",
        "                    break\n",
        "            if not found:\n",
        "                print(\n",
        "                    f\"Could not align span '{span_text}' at raw indices [{raw_start}:{raw_end+1}] \"\n",
        "                    f\"within Â±{threshold} chars in original {location}. Skipping.\"\n",
        "                )\n",
        "                cumulative_offset += len(match.group(0)) - len(span_text)\n",
        "                continue\n",
        "\n",
        "        entities.append({\n",
        "            \"start_idx\": start_idx,\n",
        "            \"end_idx\": end_idx,\n",
        "            \"location\": location,\n",
        "            \"text_span\": span_text,\n",
        "            \"label\": original_label,\n",
        "        })\n",
        "\n",
        "        marker_length = len(match.group(0)) - len(span_text)\n",
        "        cumulative_offset += marker_length\n",
        "\n",
        "    return entities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "suZGVYQvwAtE"
      },
      "outputs": [],
      "source": [
        "def process_documents(input_path, output_path, shot):\n",
        "    \"\"\"Loads data, calls LLM for inline annotation, parses output, and saves.\"\"\"\n",
        "    dev_data = load_json_data(input_path)\n",
        "    all_predictions = {}\n",
        "    processed_count = 0\n",
        "\n",
        "    for pmid, data in dev_data.items():\n",
        "        # if processed_count <=0 or processed_count>=2:\n",
        "        #   processed_count+=1\n",
        "        #   continue\n",
        "        print(f\"Processing PMID: {pmid} ({processed_count + 1}/{len(dev_data)})...\")\n",
        "        combined_entities = []\n",
        "        try:\n",
        "            # if \"metadata\" not in data or \"title\" not in data[\"metadata\"] or \"abstract\" not in data[\"metadata\"]:\n",
        "            #      print(f\"Skipping PMID {pmid} due to missing metadata, title, or abstract.\")\n",
        "            #      continue\n",
        "\n",
        "            # title = data[\"metadata\"][\"title\"]\n",
        "            # abstract = data[\"metadata\"][\"abstract\"]\n",
        "            title = data[\"title\"]\n",
        "            abstract = data[\"abstract\"]\n",
        "            title_prompt = create_few_shot_inline_prompt(title,shot,faiss_index_global,faiss_metadata_global,full_train_data,client,pmid,\"title\")\n",
        "            llm_title_output = None\n",
        "            try:\n",
        "              response = client.chat.completions.create(\n",
        "                  model=OPENAI_MODEL,\n",
        "                  messages=[{\"role\": \"user\", \"content\": title_prompt}],\n",
        "                  temperature=TEMPERATURE,\n",
        "                  top_p=TOP_P,\n",
        "                  max_tokens=MAX_TOKENS\n",
        "              )\n",
        "              llm_title_output = response.choices[0].message.content.strip()\n",
        "              print(title)\n",
        "              print(llm_title_output)\n",
        "            except Exception as e:\n",
        "                print(f\"API call failed for title PMID {pmid} title prompt\")\n",
        "\n",
        "            if llm_title_output:\n",
        "                 title_entities = parse_llm_output(llm_title_output, title, \"title\")\n",
        "                 combined_entities.extend(title_entities)\n",
        "\n",
        "            abstract_prompt = create_few_shot_inline_prompt(abstract,shot,faiss_index_global,faiss_metadata_global,full_train_data,client,pmid,\"abstract\")\n",
        "            llm_abstract_output = None\n",
        "            try:\n",
        "               response = client.chat.completions.create(\n",
        "                   model=OPENAI_MODEL,\n",
        "                   messages=[{\"role\": \"user\", \"content\": abstract_prompt}],\n",
        "                   temperature=TEMPERATURE,\n",
        "                   top_p=TOP_P,\n",
        "                   max_tokens=MAX_TOKENS\n",
        "               )\n",
        "               llm_abstract_output = response.choices[0].message.content.strip()\n",
        "               print (abstract)\n",
        "               print (llm_abstract_output)\n",
        "            except Exception as e:\n",
        "               print(f\"API call failed for abstract PMID {pmid} abstract\")\n",
        "\n",
        "            if llm_abstract_output:\n",
        "                abstract_entities = parse_llm_output(llm_abstract_output, abstract, \"abstract\")\n",
        "                combined_entities.extend(abstract_entities)\n",
        "\n",
        "            all_predictions[pmid] = {\"entities\": combined_entities}\n",
        "            processed_count += 1\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"An unexpected error occurred outside API calls while processing PMID {pmid}: {e}\")\n",
        "\n",
        "\n",
        "    try:\n",
        "        with open(output_path, 'w', encoding='utf-8') as f:\n",
        "            json.dump(all_predictions, f, ensure_ascii=False, indent=4)\n",
        "        print(f\"\\nSuccessfully processed {processed_count} documents.\")\n",
        "        print(f\"Predictions saved to {output_path}\")\n",
        "    except IOError as e:\n",
        "        print(f\"Error writing predictions to {output_path}: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qvVT3_x1x-po",
        "outputId": "6d5fd251-38da-4be6-e17d-576a0a9c1f85"
      },
      "outputs": [],
      "source": [
        "process_documents(\"/content/articles_test.json\",OUTPUT_FILE_PATH,0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r1nospYuivhy"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
